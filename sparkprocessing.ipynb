{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run command below with docker installed to download and run a spark docker instance:\n",
    "# docker run --name sparkbook -p 8881:8888 -v \"$PWD\":/home/jovyan/work jupyter/pyspark-notebook start.sh jupyter lab --LabApp.token=''\n",
    "# Next, open a web browser and navigate to localhost:8881 to enter docker container. This notebook can then be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "#Required when running from a newly created spark docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#required for nltk functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sys\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '30g'), ('spark.driver.port', '45157'), ('spark.app.name', 'SimpleApp'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1562022558122'), ('spark.driver.host', 'a3343a2ac2ad'), ('spark.executor.memory', '15g'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "#Creates Spark context and session. Sets\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '15g'), ('spark.driver.memory', '30g')])\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '30g'), ('spark.driver.port', '45157'), ('spark.app.name', 'SimpleApp'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1562022558122'), ('spark.driver.host', 'a3343a2ac2ad'), ('spark.executor.memory', '15g'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "#prints current configuration of spark context\n",
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a spark datafrom from the amazon reviews file. Using .limit(x) will subset the first x items.\n",
    "df = spark.read.csv(\"data/gaming_reviews.tsv\", sep=\"\\t\", header=True, inferSchema=True).limit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords to filter from  \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(star_rating=5, body_list=['used', 'elite', 'dangerous', 'mac', 'amazing', 'joystick', 'especially', 'love', 'twist', 'stick', 'different', 'movement', 'bindings', 'well', 'move', 'normal', 'way'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removes punctuation, adds column 'body_list' which is a list of words, and selects only the star_rating and body_list columns\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')\n",
    "\n",
    "def process_str(row):\n",
    "    \"\"\"Input: String\n",
    "    Output: List of strings without punctuation\n",
    "    \n",
    "    Removes punctuation using functions from the strings library. Performs raw string operations in C via a lookup table for maximum performance.\"\"\"\n",
    "    word_list = row.translate(str.maketrans('', '', string.punctuation)).split(' ')\n",
    "    return [x.lower() for x in word_list if x.lower() not in stop_words]\n",
    "\n",
    "process = udf(process_str, ArrayType(StringType()))\n",
    "\n",
    "df_new = df.withColumn('body_list', process(df['review_body']))\\\n",
    "        .withColumn('body_list', process(df['review_body']))\\\n",
    "        .select('star_rating', 'body_list')\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"body_list\", outputCol=\"features\")\n",
    "idf = IDF(inputCol='features', outputCol = 'idf_features')\n",
    "rf = RandomForestClassifier(labelCol='star_rating', featuresCol='idf_features', seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage one in pipeline the words are separated into count vectors\n",
    "stage1_fit = cv.fit(df_new)\n",
    "stage1_transform = stage1_fit.transform(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage two in pipeline for tf-IDF\n",
    "stage2_fit = idf.fit(stage1_transform)\n",
    "stage2_transform = stage2_fit.transform(stage1_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage three in pipeline for random forest\n",
    "stage3_fit = rf.fit(stage2_transform)\n",
    "stage3_transform = stage3_fit.transform(stage2_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "like\n",
      "return\n",
      "get\n",
      "bad\n",
      "ok\n",
      "doesnt\n",
      "sucks\n",
      "refund\n",
      "well\n",
      "one\n",
      "broke\n",
      "play\n",
      "stars\n",
      "2\n",
      "xbox\n",
      "great\n",
      "isnt\n",
      "however\n",
      "never\n"
     ]
    }
   ],
   "source": [
    "# Function that prints the top n features\n",
    "def print_top_features(model, n=10):\n",
    "    \"\"\"\n",
    "    Input: RandomForest model, number of top feature words to print.\n",
    "    Output: None. Prints top n words to console.\n",
    "    \"\"\"\n",
    "    sorted_indices = np.flip(np.argsort(model.featureImportances.toArray()))\n",
    "    for i in range(20):\n",
    "        print(stage1_fit.vocabulary[sorted_indices[i]])\n",
    "print_top_features(stage3_fit);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_idf = stage3_transform.select('star_rating', 'idf_features')\n",
    "vocabulary = np.array(stage1_fit.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x[0].toArray() for x in star_idf.select('idf_features').collect()] \n",
    "nmf_fit = nmf_model.fit(X)\n",
    "nmf_transform = nmf_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['button' 'press' 'consist' 'attack' 'br' 'toy' 'wars' 'force'\n",
      " 'lightsaber' 'triangle']\n",
      "['bundle' '1tb' 'bundles' 'includes' 'rating' 'console' 'x1' 'games'\n",
      " 'include' '399']\n",
      "['gears' 'war' 'br' 'shotgun' 'crosshairs' 'remember' 'like' 'new' 'next'\n",
      " 'original']\n",
      "['br' 'game' 'like' 'get' 'play' 'one' 'would' 'games' 'time' 'dont']\n",
      "['sonic' '62' 'sly' 'cooper' 'level' 'youll' 'boom' 'levels' 'gorgeous'\n",
      " '3d']\n",
      "['wrestlers' 'created' 'wwe' 'wrestling' 'roster' '2k15' 'theres' 'online'\n",
      " 'week' 'access']\n",
      "['music' 'songs' 'br' 'band' 'rock' 'song' 'notes' 'like' 'note' 'guitar']\n",
      "['units' 'robot' 'wars' 'z' 'series' 'z3' 'z2' 'attack' 'super' 'return']\n",
      "['backstage' 'wcw' 'assault' 'hardcore' 'match' 'women' 'challenge'\n",
      " 'opponent' 'wcws' 'matches']\n",
      "['dishonored' 'thief' 'titlebr' 'cause' 'gun' 'fps' 'get' 'vo' 'ive'\n",
      " 'bloody']\n"
     ]
    }
   ],
   "source": [
    "for i in range(nmf_fit.components_.shape[0]):\n",
    "    print(vocabulary[np.flip(np.argsort(nmf_fit.components_[i, :]))[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
