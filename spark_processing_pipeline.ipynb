{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, concat, col, lit\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************ \n",
      " [('spark.app.id', 'local-1563996930569'), ('spark.app.name', 'SimpleApp'), ('spark.rdd.compress', 'True'), ('spark.driver.port', '39331'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.driver.host', 'd6d187d3a6f1'), ('spark.ui.showConsoleProgress', 'true')] \n",
      " ************************************************************ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "print('*'*60, '\\n', sc.getConf().getAll(), '\\n', '*'*60, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = \".?,!;:\\\"'()#&\" \n",
    "rgx = sc.broadcast(re.compile('[%s]' % strip_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_str(row):\n",
    "\n",
    "    body_list = []\n",
    "    try:\n",
    "        for word in row.lower().split(): \n",
    "            word = rgx.value.sub('', word)  \n",
    "            body_list.append(word)\n",
    "        return body_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(row)\n",
    "        return ['']\n",
    "process = udf(process_str, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_bad_filter(x):\n",
    "    if x >=4: return 1\n",
    "    else: return 0\n",
    "\n",
    "good_bad = udf(good_bad_filter, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports data into spark dataframe\n",
    "#Directory: s3://amazon-reviews-pds/tsv/\n",
    "#data = sc.textFile('s3://amazon-reviews-pds/amazon_reviews_us_Video_Games_v1_00.tsv.gz')\n",
    "repartition_num = 10\n",
    "data = sc.textFile('data/amazon_reviews_us_Video_Games_v1_00.tsv')\n",
    "full_df = spark.read.csv(data, sep=\"\\t\", header=True, inferSchema=True)\n",
    "#full_df = full_df.repartition(repartition_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset selection for testing purposes\n",
    "subset_df = full_df.select('review_headline', 'review_body', 'star_rating')\\\n",
    "    .filter(full_df.star_rating != 3)\\\n",
    "    .withColumn('star_rating', good_bad('star_rating'))\\\n",
    "    .limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_col_df = subset_df.select(concat(col('review_headline'), lit(' '), col('review_body')).alias('text'), subset_df.star_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_df = two_col_df.withColumn('text_list', process(two_col_df['text']))\\\n",
    "        .select('text_list', 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"text_list\", outputCol=\"count_vec\")\n",
    "cv_fit = cv.fit(text_list_df) #need to save vocabulary from this\n",
    "cv_transform = cv_fit.transform(text_list_df)\n",
    "output_df = cv_transform.select(cv_transform.count_vec, cv_transform.star_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = spark.createDataFrame(cv_fit.vocabulary, schema=StringType())\n",
    "vocab.coalesce(1).write.format('json').save('s3://dsi-amazon-neural/vocab') #saves vocabulary as a json\n",
    "output_df.write.format('json').save('s3://dsi-amazon-neural/') #saves final dataframe in a series of json files on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
